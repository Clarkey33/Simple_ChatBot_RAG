{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be installed \n",
    "#pip install -U transformers\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#pip install bitsandbytes\n",
    "#pip install accelerate \n",
    "#conda install langchain -c conda-forge\n",
    "#conda install -c huggingface -c conda-forge datasets\n",
    "#pip install faiss-cpu\n",
    "#pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "#pip install jupyter\n",
    "#pip install PyPDF2\n",
    "#pip install -qU langchain_community pypdf\n",
    "#pip install -U sentence-transformers\n",
    "#pip install openai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.agents import AgentExecutor\n",
    "import faiss\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Document ingestion\n",
    "\n",
    "redact private infor \n",
    "text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lnitialize pdf loader\n",
    "pdf_loader = PyPDFLoader(\"hr_manual.pdf\")\n",
    "\n",
    "#load documents\n",
    "hr_manual = pdf_loader.load()\n",
    "\n",
    "#preview document\n",
    "print(f\"loaded document with {len(hr_manual)} pages from the pdf\")\n",
    "\n",
    "#preview first 500 characters\n",
    "print(hr_manual[20].page_content[:500])\n",
    "\n",
    "#data type\n",
    "print(type(hr_manual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hr_manual[0]) #this file type is immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for privacy concerns : remove metadata\n",
    "\n",
    "def sanitize_metadata(document):\n",
    "    from langchain.schema import Document\n",
    "\n",
    "    ''' redacts metadata from document'''\n",
    "\n",
    "    #create a redacted copy of meta data\n",
    "    sanitize_metadata = document.metadata.copy()\n",
    "\n",
    "    #remove sensitive data \n",
    "    sensitive_fields = ['producer','creator','author']\n",
    "\n",
    "\n",
    "    for field in sensitive_fields:\n",
    "        if field in sanitize_metadata:\n",
    "            sanitize_metadata[field] = \"[REDACTED]\"\n",
    "\n",
    "    # Create a new Document instance with the redacted metadata\n",
    "    sanitized_doc = Document(page_content=document.page_content,\n",
    "                             metadata=sanitize_metadata\n",
    "                             )\n",
    "\n",
    "    return sanitized_doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_hr_manual = [sanitize_metadata(doc) for doc in hr_manual]\n",
    "\n",
    "#Check senstive data redacted from document's metadata\n",
    "print(sanitized_hr_manual[25].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Create Embedings and Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "#initialize embedding model\n",
    "embedding_model= HuggingFaceEmbeddings(model_name=emb_model_id,\n",
    "                                       model_kwargs= model_kwargs,\n",
    "                                       encode_kwargs=encode_kwargs\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Faiss Index\n",
    "\n",
    "faiss_index = FAISS.from_documents(sanitized_hr_manual,embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Test DistilBERT for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import HuggingFacePipeline\n",
    "#from transformers import pipeline\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e98a8bbd5304c03ac07f48fdbba2bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\On3B3\\anaconda3\\envs\\prodenv3\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\On3B3\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64c3b78cdd5403bbf1b1234f3461c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edd7bd727ca4ae0ac808a0b51149784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c3b75a5544467e8d5e9de846bd7121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db958a2eea49259cb7c6b2c53f74ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load LLM model \n",
    "llm_model_id = 'distilbert-base-uncased-distilled-squad'\n",
    "tokenizer = 'distilbert-base-uncased-distilled-squad'\n",
    "\n",
    "llm_model = DistilBertForQuestionAnswering.from_pretrained(llm_model_id,\n",
    "                                                           device_map=\"cuda\",\n",
    "                                                           return_dict=True,\n",
    "                                                           torch_dtype=torch.bfloat16\n",
    "                                                           )\n",
    "\n",
    "\n",
    "#initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = llm_model.device\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = llm_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "# target is \"nice puppet\"\n",
    "target_start_index = outputs.start_logits.argmax().unsqueeze(0)\n",
    "target_end_index = outputs.end_logits.argmax().unsqueeze(0)\n",
    "\n",
    "outputs = llm_model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: a nice puppet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index:answer_end_index + 1]\n",
    "\n",
    "answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Predicted Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Flow\n",
    "\n",
    "Query → Search FAISS → Get Relevant Chunk(s) → Pass to DistilBERT → Get Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
